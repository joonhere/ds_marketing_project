import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns # ğŸ’¡ ì˜¤ë¥˜ 1: seaborn ëª¨ë“ˆ ì„í¬íŠ¸ ì¶”ê°€
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import re # ğŸ’¡ ì˜¤ë¥˜ 2: ë°ì´í„° í•„í„°ë§ ì •í™•ë„ë¥¼ ìœ„í•œ re ëª¨ë“ˆ ì„í¬íŠ¸ ì¶”ê°€

# 1. ë°ì´í„° ë¡œë“œ
file_path = 'netflix_preprocessed.csv'
# ğŸš¨ ì£¼ì˜: íŒŒì¼ ê²½ë¡œì™€ íŒŒì¼ ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”.
try:
    netflix = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"ğŸš¨ ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# 2. í•„í„°ë§ : 'korea'ì™€ ê´€ë ¨ëœ í•­ëª© í•„í„°ë§
korea_data = netflix[
    # na=FalseëŠ” NaN ê°’ì„ Falseë¡œ ì²˜ë¦¬í•˜ì—¬ í•„í„°ë§ ì˜¤ë¥˜ ë°©ì§€ (ì´ë¯¸ na=Falseê°€ ìˆë”ë¼ë„ ì•ˆì „í•œ ë°©ì‹)
    (netflix['description'].fillna('').str.contains('Korea', case=False, na=False)) |
    (netflix['title'].fillna('').str.contains('Korea', case=False, na=False)) |
    (netflix['listed_in'].fillna('').str.contains('Korean', case=False, na=False))
].copy() # ğŸ’¡ ë©”ëª¨ë¦¬ ë³µì‚¬ ë° ê²½ê³  ë°©ì§€

if korea_data.empty:
    print("ğŸš¨ ê²½ê³ : 'Korea' ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    exit()

# 3. í…ìŠ¤íŠ¸ ë°ì´í„° ê²°í•© ë° ì „ì²˜ë¦¬ (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ íŠ¹ìˆ˜ ë¬¸ì ì œê±° ì¶”ê°€)
text = ' '.join(korea_data['description'].dropna().tolist())
# raw_text = ' '.join(korea_data['description'].dropna().tolist())
# ğŸ’¡ í•œê¸€/ì˜ë¬¸/ê³µë°±ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°í•˜ì—¬ ë¶„ì„ ì •í™•ë„ í–¥ìƒ
#text = re.sub(r'[^ê°€-í£a-zA-Z\s]', '', raw_text) 

# 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë¹ˆë„ ì¶”ì¶œ
# ğŸ’¡ ì˜¤ë¥˜ 3: stop_words='englist' -> 'english'ë¡œ ìˆ˜ì •
vectorizer = CountVectorizer(stop_words='english') # , token_pattern=r'(?u)\b\w\w+\b') # ë‘ ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì¶”ì¶œ
word_matrix = vectorizer.fit_transform([text])
# ğŸ’¡ ì˜¤ë¥˜ 4: wors_freq -> word_freq (ë³€ìˆ˜ëª… ì˜¤íƒ€ ìˆ˜ì •)
word_freq = word_matrix.toarray().flatten() 

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ìƒì„±
word_df = pd.DataFrame({
    'word': vectorizer.get_feature_names_out(), 
    'freq': word_freq
})

# ğŸ’¡ ì˜¤ë¥˜ 5: sort_valuse -> sort_valuesë¡œ ìˆ˜ì •
# ğŸ’¡ ì˜¤ë¥˜ 6: by='frequency' -> by='freq'ë¡œ ì»¬ëŸ¼ëª… ì¼ì¹˜ ìˆ˜ì •
# ğŸ’¡ ì˜¤ë¥˜ 7: top_wird_df -> top_words_df (ë³€ìˆ˜ëª… ì˜¤íƒ€ ìˆ˜ì •)
word_df_sorted = word_df.sort_values(by='freq', ascending=False)
top_words_df = word_df_sorted.head(10)
print(top_words_df)

# 5. ì‹œê°í™”: ìƒìœ„ ë‹¨ì–´ë¥¼ seabornì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”
plt.figure(figsize=(10,6))
# ğŸ’¡ ì˜¤ë¥˜ 8: platte='viridis' -> palette='viridis'ë¡œ ì˜¤íƒ€ ìˆ˜ì •
sns.barplot(data=top_words_df, x='freq', y='word', palette='viridis')
plt.title('Top 10 Korean Words in Descriptions Related to Korea')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

# 6. ì‹œê°í™”: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(
    dict(zip(word_df_sorted['word'], word_df_sorted['freq'])) # ğŸ’¡ ìˆ˜ì •: ì •ë ¬ëœ word_df_sorted ì‚¬ìš©
)

# ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()