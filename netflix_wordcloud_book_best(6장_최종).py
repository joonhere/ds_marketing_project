import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import re 

# --- 0. ìƒìˆ˜ ì •ì˜ (ì½”ë“œì˜ ìœ ì—°ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í™•ë³´) ---
COUNTRY_COLUMN = 'country'
GENRE_COLUMN = 'listed_in'
TITLE_COLUMN = 'title'
DESCRIPTION_COLUMN = 'description'
CUSTOM_STOP_WORDS = {'series', 'film', 'movie', 'show', 'story', 'life', 'new', 'world', 'us', 'korean', 'korea', 'drama', 'kdrama'} # ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´

# --- 1. ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§ í•¨ìˆ˜ ---
def load_and_filter_data(
    file_path: str, 
    filter_keyword: str, 
    cols_to_check: list[str]
) -> pd.DataFrame | None:     # ğŸš¨ Optional ëŒ€ì‹  'íƒ€ì… | None' ì‚¬ìš©
    """
    CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì§€ì •ëœ ì»¬ëŸ¼ë“¤ì—ì„œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” í–‰ì„ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(file_path)
        
        # í•„í„°ë§ ì¡°ê±´ ì¡°í•©: ì—¬ëŸ¬ ì»¬ëŸ¼ì— ëŒ€í•´ OR ì¡°ê±´ì„ ì ìš©
        filter_condition = False
        for col in cols_to_check:
            # ğŸ’¡ Null ê°’ ì²˜ë¦¬ ë° í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            if col in df.columns:
                filter_condition = filter_condition | df[col].fillna('').str.contains(filter_keyword, case=False, na=False)
            else:
               print(f"ğŸš¨ ê²½ê³ : ì»¬ëŸ¼ '{col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì»¬ëŸ¼ì€ í•„í„°ë§ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
        
        filtered_df = df[filter_condition].copy()
        
        if filtered_df.empty:    # ë§Œì•½ filtered_df ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆë‹¤ë©´: True ì„
            print(f"ğŸš¨ ê²½ê³ : '{filter_keyword}' ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        return filtered_df    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    
    except FileNotFoundError:
        print(f"ğŸš¨ ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

# --- 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ í•¨ìˆ˜ ---
def analyze_word_frequency(
		df: pd.DataFrame, 
		text_col: str, 
		custom_stopwords: set[str]
) -> tuple[pd.DataFrame, str]:
    """
    í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì „ì²˜ë¦¬í•˜ê³  CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë¹ˆë„ë¥¼ ì¶”ì¶œ.
    """
    # ë°ì´í„° í”„ë ˆì„ì—ì„œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©
    raw_text = ' '.join(df[text_col].dropna().tolist())
    # ğŸ’¡ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©ëœ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ì •ì œ
    text_clean = re.sub(r'[^ê°€-í£a-zA-Z\s]', ' ', raw_text).lower()
    
    # ì˜ë¬¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ì— ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì¶”ê°€  
    # ğŸš¨ ìˆ˜ì •: frozensetì„ set()ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ updateê°€ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    all_stopwords = set(CountVectorizer(stop_words='english').get_stop_words())
    # ì´ì œ all_stopwordsëŠ” ì¼ë°˜ setì´ë¯€ë¡œ updateê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    all_stopwords.update(custom_stopwords)
    
    # CountVectorizer()ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 2ë‹¨ì–´ë¶€í„° ì¶œë ¥ëœë‹¤ r"(?u)\b\w\w+\b" ë‚´í¬í•¨.
    # ğŸ’¡ í† í° íŒ¨í„´(token_pattern=r'(?u)\b\w\w+\b')ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ëª…ì‹œì ìœ¼ë¡œ ì¸ì‹í•˜ê¸° ìœ„í•´ 
    vectorizer = CountVectorizer(
        # ğŸš¨ ìˆ˜ì •: set í˜•íƒœì¸ all_stopwordsë¥¼ list()ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬í•©ë‹ˆë‹¤.
        stop_words=list(all_stopwords),
        token_pattern=r'(?u)\b\w\w+\b' 
    )
    
    # í•©ì³ì§„ ë¶ˆìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë¹ˆë„ í–‰ë ¬ì„ ìƒì„±
    word_matrix = vectorizer.fit_transform([text_clean])
    word_freq = word_matrix.toarray().flatten()    # í–‰ë ¬ì„ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ flatten /Ëˆflatn/ ë‹¨ì¡°ë¡­ê²Œí•˜ë‹¤
    
		# ë¹ˆë„ ê²°ê³¼ë¥¼ dataFrameìœ¼ë¡œ ì •ë¦¬(word_df)
    word_df = pd.DataFrame({
        'word': vectorizer.get_feature_names_out(), 
        'freq': word_freq
    }).sort_values(by='freq', ascending=False)
    
    return word_df, text_clean
    
# --- 3. ì‹œê°í™” í•¨ìˆ˜ ---
def visualize_results(word_df: pd.DataFrame, top_n: int, title_prefix: str) -> None:
    """
    ìƒìœ„ ë‹¨ì–´ì— ëŒ€í•œ Bar plotê³¼ WordCloudë¥¼ ìƒì„±í•˜ê³  í‘œì‹œí•©ë‹ˆë‹¤.
    """
    top_words_df = word_df.head(top_n)

    # 3.1 Bar Plot ì‹œê°í™”
    plt.figure(figsize=(10,6))
    sns.barplot(
	    data=top_words_df, 
	    x='freq', 
	    y='word', 
	    hue='word',         # âœ… y ë³€ìˆ˜ì¸ 'word'ë¥¼ hueì— í• ë‹¹
	    palette='viridis',
	    legend=False        # âœ… ë¶ˆí•„ìš”í•œ ë²”ë¡€ë¥¼ ìˆ¨ê¹€
		)
    plt.title(f'{title_prefix} Top {top_n} Words in Descriptions', fontsize=16)
    plt.xlabel('Frequency')
    plt.ylabel('Word')
    plt.show()

    # 3.2 WordCloud ì‹œê°í™”
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(
        dict(zip(word_df['word'], word_df['freq']))
    )
    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'{title_prefix} WordCloud', fontsize=16)
    plt.axis('off')
    plt.show()

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    
    # ğŸ’¡ ë¶„ì„í•  ëŒ€ìƒì„ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ (í•˜ë“œì½”ë”© ì œê±°)
    CSV_FILE = 'netflix_preprocessed.csv'
    FILTER_KEYWORD = 'Korea'
    COLUMNS_TO_CHECK = [DESCRIPTION_COLUMN, TITLE_COLUMN, GENRE_COLUMN]
    TOP_N_WORDS = 10
    
    print(f"--- ë„·í”Œë¦­ìŠ¤ '{FILTER_KEYWORD}' ì½˜í…ì¸  ë¶„ì„ ì‹œì‘ ---")
    
    # 1. ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
    korea_df = load_and_filter_data(
        file_path=CSV_FILE,
        filter_keyword=FILTER_KEYWORD,
        cols_to_check=COLUMNS_TO_CHECK
    )
    
    if korea_df is None:
        exit()
        
    print(f"âœ… '{FILTER_KEYWORD}' ê´€ë ¨ ì½˜í…ì¸  ì´ {len(korea_df)}ê°œ ë°œê²¬.")
    print(korea_df)

    # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¶„ì„
    word_freq_df, _ = analyze_word_frequency(
        df=korea_df, 
        text_col=DESCRIPTION_COLUMN,
        custom_stopwords=CUSTOM_STOP_WORDS
    )
    print(word_freq_df)

    # 3. ì‹œê°í™”
    visualize_results(
        word_df=word_freq_df, 
        top_n=TOP_N_WORDS, 
        title_prefix=f'KOREAN Netflix Content'
    )
    
    print("--- ë¶„ì„ ì™„ë£Œ ---")