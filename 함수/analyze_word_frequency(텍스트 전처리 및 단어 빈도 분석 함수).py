# --- 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ í•¨ìˆ˜ ---
def analyze_word_frequency(
		df: pd.DataFrame, 
		text_col: str, 
		custom_stopwords: set[str]
) -> tuple[pd.DataFrame, str]:
    """
    í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì „ì²˜ë¦¬í•˜ê³  CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë¹ˆë„ë¥¼ ì¶”ì¶œ.
    """
    # ë°ì´í„° í”„ë ˆì„ì—ì„œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©
    raw_text = ' '.join(df[text_col].dropna().tolist())
    # ğŸ’¡ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©ëœ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ì •ì œ
    text_clean = re.sub(r'[^ê°€-í£a-zA-Z\s]', ' ', raw_text).lower()
    
    # ì˜ë¬¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ì— ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì¶”ê°€  
    # ğŸš¨ ìˆ˜ì •: frozensetì„ set()ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ updateê°€ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    all_stopwords = set(CountVectorizer(stop_words='english').get_stop_words())
    # ì´ì œ all_stopwordsëŠ” ì¼ë°˜ setì´ë¯€ë¡œ updateê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    all_stopwords.update(custom_stopwords)
    
    # CountVectorizer()ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 2ë‹¨ì–´ë¶€í„° ì¶œë ¥ëœë‹¤ r"(?u)\b\w\w+\b" ë‚´í¬í•¨.
    # ğŸ’¡ í† í° íŒ¨í„´(token_pattern=r'(?u)\b\w\w+\b')ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ëª…ì‹œì ìœ¼ë¡œ ì¸ì‹í•˜ê¸° ìœ„í•´ 
    vectorizer = CountVectorizer(
        # ğŸš¨ ìˆ˜ì •: set í˜•íƒœì¸ all_stopwordsë¥¼ list()ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬í•©ë‹ˆë‹¤.
        stop_words=list(all_stopwords),
        token_pattern=r'(?u)\b\w\w+\b' 
    )
    
    # í•©ì³ì§„ ë¶ˆìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë¹ˆë„ í–‰ë ¬ì„ ìƒì„±
    word_matrix = vectorizer.fit_transform([text_clean])
    word_freq = word_matrix.toarray().flatten()    # í–‰ë ¬ì„ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ flatten /Ëˆflatn/ ë‹¨ì¡°ë¡­ê²Œí•˜ë‹¤
    
		# ë¹ˆë„ ê²°ê³¼ë¥¼ dataFrameìœ¼ë¡œ ì •ë¦¬(word_df)
    word_df = pd.DataFrame({
        'word': vectorizer.get_feature_names_out(), 
        'freq': word_freq
    }).sort_values(by='freq', ascending=False)
    
    return word_df, text_clean